{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 552 - Generative AI\n",
    "## Homework 2 - Kevin Metzler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "penguins = pd.read_csv(url)\n",
    "\n",
    "# Filter the dataset for Adelie and Gentoo species\n",
    "penguins = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]\n",
    "\n",
    "# Drop rows with missing values\n",
    "penguins.dropna(inplace=True)\n",
    "\n",
    "# Encode the species column\n",
    "penguins['species'] = penguins['species'].map({'Adelie': 0, 'Gentoo': 1})\n",
    "\n",
    "# Define features and target\n",
    "X = penguins.drop(columns=['species'])\n",
    "y = penguins['species']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate Naive Bayes model\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "nb_report = classification_report(y_test, y_pred_nb)\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate Logistic Regression model\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_report = classification_report(y_test, y_pred_lr)\n",
    "\n",
    "# Print the results\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "print(\"Naive Bayes Classification Report:\\n\", nb_report)\n",
    "print(\"Logistic Regression Accuracy:\", lr_accuracy)\n",
    "print(\"Logistic Regression Classification Report:\\n\", lr_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes model on training data\n",
    "y_train_pred_nb = nb_model.predict(X_train)\n",
    "nb_train_accuracy = accuracy_score(y_train, y_train_pred_nb)\n",
    "\n",
    "# Evaluate Logistic Regression model on training data\n",
    "y_train_pred_lr = lr_model.predict(X_train)\n",
    "lr_train_accuracy = accuracy_score(y_train, y_train_pred_lr)\n",
    "\n",
    "# Print the results\n",
    "print(\"Naive Bayes Training Accuracy:\", nb_train_accuracy)\n",
    "print(\"Naive Bayes Test Accuracy:\", nb_accuracy)\n",
    "print(\"Logistic Regression Training Accuracy:\", lr_train_accuracy)\n",
    "print(\"Logistic Regression Test Accuracy:\", lr_accuracy)\n",
    "\n",
    "# Compare the performance\n",
    "if nb_accuracy > lr_accuracy:\n",
    "    print(\"Naive Bayes model performs better on the test dataset.\")\n",
    "elif nb_accuracy < lr_accuracy:\n",
    "    print(\"Logistic Regression model performs better on the test dataset.\")\n",
    "else:\n",
    "    print(\"Both models perform equally well on the test dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Both the Naive Bayes and Logistic Regression models achieved an accuracy of 1.0 on the test dataset. This indicates that both models are highly effective in classifying the two penguin species.\n",
    "\n",
    "### Conclusion\n",
    "Both the Naive Bayes and Logistic Regression models perform equally well in classifying the two penguin species based on the accuracy metric. Given that both models achieve perfect accuracy, there is no clear winner in terms of distinguishing between the two species. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate AUC for Naive Bayes model on training and test datasets\n",
    "nb_train_auc = roc_auc_score(y_train, nb_model.predict_proba(X_train)[:, 1])\n",
    "nb_test_auc = roc_auc_score(y_test, nb_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Calculate AUC for Logistic Regression model on training and test datasets\n",
    "lr_train_auc = roc_auc_score(y_train, lr_model.predict_proba(X_train)[:, 1])\n",
    "lr_test_auc = roc_auc_score(y_test, lr_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Print the AUC values\n",
    "print(\"Naive Bayes Training AUC:\", nb_train_auc)\n",
    "print(\"Naive Bayes Test AUC:\", nb_test_auc)\n",
    "print(\"Logistic Regression Training AUC:\", lr_train_auc)\n",
    "print(\"Logistic Regression Test AUC:\", lr_test_auc)\n",
    "\n",
    "# Interpret the AUC values\n",
    "if nb_test_auc > lr_test_auc:\n",
    "    print(\"Naive Bayes model is more effective based on the AUC metric.\")\n",
    "elif nb_test_auc < lr_test_auc:\n",
    "    print(\"Logistic Regression model is more effective based on the AUC metric.\")\n",
    "else:\n",
    "    print(\"Both models are equally effective based on the AUC metric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model\n",
    "- **Training AUC:** 1.0\n",
    "- **Test AUC:** 1.0\n",
    "\n",
    "### Logistic Regression Model\n",
    "- **Training AUC:** 1.0\n",
    "- **Test AUC:** 1.0\n",
    "\n",
    "### Insights\n",
    "Both the Naive Bayes and Logistic Regression models have achieved perfect AUC scores of 1.0 on both the training and test datasets. This indicates that both models are excellent at discriminating between the two penguin species. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate predicted probabilities for both models\n",
    "nb_probs = nb_model.predict_proba(X_test)[:, 1]\n",
    "lr_probs = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create deciles\n",
    "nb_deciles = pd.qcut(nb_probs, 10, labels=False)\n",
    "lr_deciles = pd.qcut(lr_probs, 10, labels=False)\n",
    "\n",
    "# Calculate Lift and Gain for each decile\n",
    "def calculate_lift_gain(y_true, probs, deciles):\n",
    "    data = pd.DataFrame({'y_true': y_true, 'probs': probs, 'deciles': deciles})\n",
    "    data = data.sort_values(by='probs', ascending=False)\n",
    "    \n",
    "    total_positives = data['y_true'].sum()\n",
    "    data['cumulative_positives'] = data['y_true'].cumsum()\n",
    "    data['cumulative_total'] = np.arange(1, len(data) + 1)\n",
    "    \n",
    "    data['gain'] = data['cumulative_positives'] / total_positives\n",
    "    data['lift'] = data['gain'] / (data['cumulative_total'] / len(data))\n",
    "    \n",
    "    lift = data.groupby('deciles')['lift'].last().values\n",
    "    gain = data.groupby('deciles')['gain'].last().values\n",
    "    \n",
    "    return lift, gain\n",
    "\n",
    "nb_lift, nb_gain = calculate_lift_gain(y_test, nb_probs, nb_deciles)\n",
    "lr_lift, lr_gain = calculate_lift_gain(y_test, lr_probs, lr_deciles)\n",
    "\n",
    "# Plot Lift and Gain charts\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "deciles = np.arange(1, 11)\n",
    "\n",
    "ax1.set_xlabel('Deciles')\n",
    "ax1.set_ylabel('Lift', color='tab:blue')\n",
    "ax1.plot(deciles, nb_lift, label='Naive Bayes Lift', color='tab:blue', marker='o')\n",
    "ax1.plot(deciles, lr_lift, label='Logistic Regression Lift', color='tab:cyan', marker='o')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Gain', color='tab:red')\n",
    "ax2.plot(deciles, nb_gain, label='Naive Bayes Gain', color='tab:red', marker='x')\n",
    "ax2.plot(deciles, lr_gain, label='Logistic Regression Gain', color='tab:orange', marker='x')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "plt.title('Lift and Gain Charts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift and Gain Charts\n",
    "- **Lift:**\n",
    "    - Both models show similar lift values across deciles, with the highest lift observed in the top deciles.\n",
    "- **Gain:**\n",
    "    - Both models show similar gain values, with the highest gain observed in the bottom deciles.\n",
    "\n",
    "### Conclusion\n",
    "Both the Naive Bayes and Logistic Regression models perform equally well in classifying the two penguin species based on the accuracy, AUC, and Lift/Gain charts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Both the Naive Bayes and Logistic Regression models achieved an accuracy of 1.0 on the test dataset. \n",
    "\n",
    "### AUC (Area Under the Curve)\n",
    "Both models also achieved perfect AUC scores of 1.0 on both the training and test datasets.\n",
    "\n",
    "### Lift and Gain Charts\n",
    "- **Lift:**\n",
    "    - Both models show similar lift values across deciles, with the highest lift observed in the top deciles.\n",
    "- **Gain:**\n",
    "    - Both models show similar gain values, with the highest gain observed in the bottom deciles.\n",
    "\n",
    "### Conclusion\n",
    "Based on the accuracy, AUC, and Lift/Gain charts, both the Naive Bayes and Logistic Regression models perform equally well in classifying the two penguin species. Both models achieve perfect scores in all metrics, indicating that they are highly effective and we can't choose a better model based on these metrics alone.\n",
    "\n",
    "However, in practical applications, other factors such as model interpretability, training time, and computational resources might influence the choice of model. I prefer Logistic Regression for its interpretability, while Naive Bayes can be better in scenarios with smaller datasets or when we can assume feature independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X_mnist, y_mnist = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_mnist = scaler.fit_transform(X_mnist)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_mnist, X_test_mnist, y_train_mnist, y_test_mnist = train_test_split(X_mnist, y_mnist, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Naive Bayes model\n",
    "nb_model_mnist = GaussianNB()\n",
    "nb_model_mnist.fit(X_train_mnist, y_train_mnist)\n",
    "\n",
    "# Predict and evaluate Naive Bayes model\n",
    "y_pred_nb_mnist = nb_model_mnist.predict(X_test_mnist)\n",
    "nb_accuracy_mnist = accuracy_score(y_test_mnist, y_pred_nb_mnist)\n",
    "nb_report_mnist = classification_report(y_test_mnist, y_pred_nb_mnist)\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "lr_model_mnist = LogisticRegression(max_iter=1000)\n",
    "lr_model_mnist.fit(X_train_mnist, y_train_mnist)\n",
    "\n",
    "# Predict and evaluate Logistic Regression model\n",
    "y_pred_lr_mnist = lr_model_mnist.predict(X_test_mnist)\n",
    "lr_accuracy_mnist = accuracy_score(y_test_mnist, y_pred_lr_mnist)\n",
    "lr_report_mnist = classification_report(y_test_mnist, y_pred_lr_mnist)\n",
    "\n",
    "# Print the results\n",
    "print(\"Naive Bayes Accuracy on MNIST:\", nb_accuracy_mnist)\n",
    "print(\"Naive Bayes Classification Report on MNIST:\\n\", nb_report_mnist)\n",
    "print(\"Logistic Regression Accuracy on MNIST:\", lr_accuracy_mnist)\n",
    "print(\"Logistic Regression Classification Report on MNIST:\\n\", lr_report_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on MNIST Dataset\n",
    "\n",
    "#### Naive Bayes (Generative Model)\n",
    "- **Accuracy:** 0.53\n",
    "\n",
    "#### Logistic Regression (Discriminative Model)\n",
    "- **Accuracy:** 0.92\n",
    "\n",
    "#### MNIST Dataset\n",
    "- **Naive Bayes:** The generative model performs poorly on the MNIST dataset with an accuracy of 0.53. This is likely due to the high dimensionality and complexity of the image data, which violates the assumption of feature independence. \n",
    "- **Logistic Regression:** The discriminative model performs significantly better with an accuracy of 0.92. Logistic Regression is better suited for high-dimensional data and can capture complex relationships between features.\n",
    "\n",
    "#### Penguin Dataset\n",
    "- **Naive Bayes and Logistic Regression:** Both models achieve perfect accuracy (1.0) on the penguin dataset. This dataset is simpler with fewer features and clear distinctions between the two species, making it easier for both models to perform well.\n",
    "\n",
    "### Conclusion\n",
    "- **Generative Models (Naive Bayes):** Perform well on simpler datasets with fewer features and clear distinctions but struggle with high-dimensional and complex data like images. \n",
    "- **Discriminative Models (Logistic Regression):** Perform well on both simple and complex datasets, making them more versatile for various types of data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
